
# Week 2: From Raw Dataset â†’ Clean ML-Ready Pipeline

## Problem Statement

Build a reusable, end-to-end data preprocessing pipeline that turns messy real-world data into a model-ready dataset. This project demonstrates the foundation of professional ML work: understanding, cleaning, and transforming raw data.

## ðŸŽ¯ Objectives

- **Exploratory Data Analysis (EDA)**: Understand data structure, distributions, and anomalies
- **Preprocessing Pipeline**: Build reusable, scalable data transformation code
- **Feature Engineering**: Handle missing values, outliers, and categorical variables
- **Model Validation**: Train-test split and basic model evaluation

## ðŸ“¦ Project Scope

### Input
A messy real-world dataset with:
- Missing values
- Categorical variables
- Outliers
- Inconsistent data types

### Output
- Cleaned, processed dataset
- Reusable preprocessing code (sklearn Pipeline)
- Data quality report
- Train-test split ready for modeling

## ðŸ”§ What's Included

### 1. **Exploratory Data Analysis** (`notebooks/eda.ipynb`)
- Data shape and data types
- Missing value patterns
- Statistical distributions
- Correlation analysis
- Outlier detection

### 2. **Preprocessing Pipeline** (`src/preprocessing.py`)
```python
class DataPreprocessor:
    """Reusable preprocessing pipeline using sklearn."""
    
    - Handle missing values (imputation strategies)
    - Encode categorical variables (OneHot / Ordinal)
    - Scale numerical features (StandardScaler / MinMaxScaler)
    - Detect and handle outliers
    - Train-test split
```

### 3. **Requirements** (`requirements.txt`)
- pandas, numpy: Data manipulation
- scikit-learn: ML preprocessing & modeling
- matplotlib, seaborn: Visualization
- jupyter, ipython: Notebooks

### 4. **Folder Structure**
```
week-2/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Original messy data
â”‚   â””â”€â”€ processed/        # Cleaned, transformed data
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ eda.ipynb        # Exploratory analysis
â”œâ”€â”€ src/
â”‚   â””â”€â”€ preprocessing.py  # Reusable pipeline
â”œâ”€â”€ requirements.txt      # Project dependencies
â””â”€â”€ README.md            # This file
```

## ðŸ§  Key Concepts Demonstrated

âœ… **EDA**: Understand data before modeling  
âœ… **Feature Scaling**: Normalize numerical features  
âœ… **Categorical Encoding**: Transform text to numbers  
âœ… **Missing Data Strategy**: Imputation vs deletion  
âœ… **Train-Test Split**: Prevent data leakage  
âœ… **Reproducibility**: Reusable, documented code  
âœ… **Sklearn Pipelines**: Professional ML best practices  

## ðŸš€ Getting Started

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Run EDA Notebook
```bash
jupyter notebook notebooks/eda.ipynb
```

### 3. Use the Preprocessing Pipeline
```python
from src.preprocessing import DataPreprocessor
import pandas as pd

# Load data
df = pd.read_csv('data/raw/your_dataset.csv')

# Initialize preprocessor
preprocessor = DataPreprocessor(test_size=0.2, random_state=42)

# Split data
X_train, X_test, y_train, y_test = preprocessor.split_data(X, y)

# Fit and transform
X_train_processed = preprocessor.fit_transform(X_train, y_train)
X_test_processed = preprocessor.transform(X_test)
```

## ðŸ“Š Expected Results

- âœ… Clean dataset with no missing values
- âœ… Properly scaled numerical features
- âœ… Encoded categorical variables
- âœ… Train-test split (80-20)
- âœ… Documented EDA notebook
- âœ… Baseline model evaluation

## ðŸŽ“ Learnings

1. **Data Quality Matters**: 80% of ML work is preprocessing
2. **Reproducibility**: Use pipelines for consistent results
3. **Feature Engineering**: Domain knowledge + data understanding
4. **Train-Test Split**: Prevent overfitting from day one
5. **Documentation**: Code that works + code others understand

## ðŸ“š Resources Used

- [scikit-learn Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)
- [Handling Missing Data](https://pandas.pydata.org/docs/user_guide/missing_data.html)
- [Feature Scaling](https://scikit-learn.org/stable/modules/preprocessing.html)

## ðŸ”— Related Topics

- Week 1: System Setup & Repository Structure
- Week 3: Model Training & Evaluation (coming next)
- Week 4+: Advanced ML techniques

---

**Status**: ðŸŸ¡ In Progress  
**Started**: Jan 9, 2026  

**Completed**: Jan 14, 2026
**Language**: Python 3.8+
**Framework**: scikit-learn, pandas, jupyter (Colab)

## âœ… v1 Completion Status

**DONE**: Full end-to-end preprocessing pipeline

### Pipeline Execution Results
- **Dataset**: Titanic (891 samples, 15 raw features)
- **Train-Test Split**: 712 training, 179 testing samples
- **Missing Value Handling**: Imputed mean for numeric, mode for categorical
- **Categorical Encoding**: OneHotEncoder â†’ 28 final features
- **Numerical Scaling**: StandardScaler applied
- **Model Training**: RandomForest classifier - **100% accuracy** on test set
- **Implementation**: Google Colab notebook (fully reproducible)

### Key Achievements
âœ… Download & load real dataset  
âœ… Complete preprocessing pipeline (7 steps)  
âœ… Train-test split (80-20)  
âœ… Handle missing values (mean imputation)  
âœ… Encode categorical variables (OneHotEncoder)  
âœ… Scale numerical features (StandardScaler)  
âœ… Train model and evaluate performance  
âœ… 100% accuracy on test set  

### How to Access
1. **Colab Notebook**: `notebooks/exploration.ipynb` - Full pipeline with outputs
2. **Run Locally**: Clone repo, install requirements, adapt notebook code
**Language**: Python 3.8+  
**Framework**: scikit-learn, pandas
